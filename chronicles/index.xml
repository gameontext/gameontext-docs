<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Appendix: The Chronicles on Game On! Adventures with microservices</title><link>/chronicles/</link><description>Recent content in Appendix: The Chronicles on Game On! Adventures with microservices</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 03 Mar 2020 17:29:15 -0500</lastBuildDate><atom:link href="/chronicles/index.xml" rel="self" type="application/rss+xml"/><item><title>The First Pass</title><link>/chronicles/1-first-pass.html</link><pubDate>Tue, 03 Mar 2020 17:29:15 -0500</pubDate><guid>/chronicles/1-first-pass.html</guid><description>Our first attempt was typical stand-things-up-in-a hurry hacking. This was a bit of a hybrid: we did start this as separate runtime services, but they were built in more monolithic chunks.
At this early early stage, it was a bunch of us (Java developers) standing these services up on our local machines. We were able to share code (thank you git), but relied on gradle and and local WDT dev/test environments to make progress without having these services deployed out into cloud-space.</description></item><item><title>Automation! Clouds!</title><link>/chronicles/2-cloud-automation.html</link><pubDate>Tue, 03 Mar 2020 17:29:15 -0500</pubDate><guid>/chronicles/2-cloud-automation.html</guid><description>Automated build pipelines arrived! and Docker containers out in the cloud! Thankfully for those of us doing most of the dev work, the creation, testing of the delivery pipeline was pretty transparent. The build pipeline is triggered by a commit to the master branches of our repositories, which means no change to our usual workflow.
A proxy was added to help route requests between the services and docker containers, and to give us a seamless domain space (gameontext.</description></item><item><title>Freeing the Front-End</title><link>/chronicles/3-web-front-end.html</link><pubDate>Wed, 08 Nov 2023 18:24:05 -0500</pubDate><guid>/chronicles/3-web-front-end.html</guid><description>We have a few Kate&amp;#8217;s on the team, and one of them is an avowed FED (that&amp;#8217;s a Front-End Developer, not a federal employee). She was happy to work on improving our front end web application, but really, she couldn&amp;#8217;t care less about Java. We realized, in fact, that there was no good reason to keep making her care about Java.
We therefore split the web stuff into its own (small, simple) container, using nginx to serve the static content.</description></item><item><title>Something other than Java!</title><link>/chronicles/4-polyglot.html</link><pubDate>Tue, 03 Mar 2020 17:29:15 -0500</pubDate><guid>/chronicles/4-polyglot.html</guid><description>Our team has nothing against JavaScript, but Java is what we know. When attempting something big and new, we felt it was a good idea to constrain the number of moving parts in the air. In this case, we stuck with a language we were familiar with for the first core services.
The Node.js room brought JavaScipt to the backend. More will follow.</description></item><item><title>Show the API</title><link>/chronicles/5-swagger.html</link><pubDate>Tue, 03 Mar 2020 17:29:15 -0500</pubDate><guid>/chronicles/5-swagger.html</guid><description>Swagger documentation!
Player and Concierge both have a REST API, and the Swagger container hosts and serves the document that presents both APIs together in one view.
The bulk of the traffic flowing around in the game is over WebSockets, and that is difficult for Swagger to document. For those interested in the real low-level details, we wrote down and iterated on the WebSocket protocol in a Box note. The result is documented here.</description></item><item><title>Breaking down the Player service</title><link>/chronicles/6-player-explodes.html</link><pubDate>Tue, 03 Mar 2020 17:29:15 -0500</pubDate><guid>/chronicles/6-player-explodes.html</guid><description>Over time, the Player service, which was filling multiple roles, felt more and more out of place. It was already cut down once, when we moved the webapp out, but as we resolved some websocket traffic management issues in the mediator, it became clear that we had two very different work patterns within the same service, one dealing only with CRUD operations, and one wielding data flowing over long-running connections. The two workloads have fundamentally different behaviors and scaling characteristics, and should not be housed in the same service.</description></item><item><title>Config management</title><link>/chronicles/7-etcd.html</link><pubDate>Tue, 03 Mar 2020 17:29:15 -0500</pubDate><guid>/chronicles/7-etcd.html</guid><description>So, at this point, we had a good fistful of services going through deployment pipelines, and we were fresh from the effort of adding a new one (for mediator). Managing environment variables across several build pipelines is a pain: lots of clicking lots of boxes to get to lots of little perfect UI views.
We moved runtime environment variables particular to our production docker environment out into etcd. This simplified what we needed to configure in our build pipelines, and made it easier for containers to pick up new values when they were restarted (no dynamic reconfiguration yet).</description></item><item><title>Buh-bye to Concierge!</title><link>/chronicles/8-bye-concierge.html</link><pubDate>Tue, 03 Mar 2020 17:29:15 -0500</pubDate><guid>/chronicles/8-bye-concierge.html</guid><description>Concierge was such a cute service! It was supposed to help players find their way from room to room. But it&amp;#8217;s name was confusing (was it a map or a traffic director?). It&amp;#8217;s interaction pattern with mediator also introduced a single point of failure: if the Concierge was down, the Players were pretty much stuck wherever they were, with a fallback to First Room.
We designed a new Map service, with a much better API for adding and removing rooms from the map.</description></item><item><title>Authorized operations</title><link>/chronicles/9-map-auth-hmac.html</link><pubDate>Tue, 03 Mar 2020 17:29:15 -0500</pubDate><guid>/chronicles/9-map-auth-hmac.html</guid><description>We added signed header-based security to the Map service APIs.
Adding security after the fact is a bad idea. Just sayin'. We managed it, but it was messy, and required coordinated rollouts across our services to get the support for Signed JWTs and signed HMACs working the way we wanted.
The shared library we built for managing signature verification did make this easier for Java.</description></item><item><title>More Rooms!</title><link>/chronicles/10-more-rooms.html</link><pubDate>Tue, 03 Mar 2020 17:29:15 -0500</pubDate><guid>/chronicles/10-more-rooms.html</guid><description>Rooms written in Go, another room in JavaScript, and two more in Java. We learned lots as other people were able to try adding their own rooms, especially as some of them were trying to build their rooms while the Map/Concierge switch was in flight. Ouch.
In the end, I realize that my experience was different from what a new developer today would experience. We were building on shifting sand, they will be building on a more stable API set.</description></item><item><title>Swept away...</title><link>/chronicles/11-the-sweep.html</link><pubDate>Tue, 03 Mar 2020 17:29:15 -0500</pubDate><guid>/chronicles/11-the-sweep.html</guid><description>Our first non-player character! The Sweep is in early days, so we don&amp;#8217;t know in what directions it will grow, but it will serve an important function: keeping the map alive and vital by pruning sick rooms.</description></item><item><title>Rec Room Gets the Cold Shoulder</title><link>/chronicles/12-room-isolation.html</link><pubDate>Tue, 03 Mar 2020 17:29:15 -0500</pubDate><guid>/chronicles/12-room-isolation.html</guid><description>Go on Now, Go! Walk out the Door.. (don&amp;#8217;t turn around, now&amp;#8230;&amp;#8203;)
As user-created rooms were added to the Map, we started thinking about treating our rooms like everybody else&amp;#8217;s. Up until this point, our rooms have cheated (a little), as they&amp;#8217;ve been able to directly talk to the core services. But no more! For we have kicked the rooms out of the core services club, and told them they can no longer be members.</description></item><item><title>Auth Goes Its Own Way</title><link>/chronicles/13-auth-service.html</link><pubDate>Tue, 03 Mar 2020 17:29:15 -0500</pubDate><guid>/chronicles/13-auth-service.html</guid><description>Remember way back in the beginning when we had only 3 services total and the Player service did all the things that ever needed to be done regarding players? Well, "all the things" included interacting with Social Login services to authenticate the Player and issue an access token (JWT).
As we grew to support additional Social Login services, the dependency management for Player become problematic. 3rd party libraries would require dependencies that would conflict with others required by our core Player entity store function.</description></item><item><title>Pandemonium! Events!</title><link>/chronicles/14-events.html</link><pubDate>Tue, 03 Mar 2020 17:29:15 -0500</pubDate><guid>/chronicles/14-events.html</guid><description>There are some things in the game that really aren&amp;#8217;t good to handle with REST APIs. Reacting to events is a much more efficient way of propagating changes through the system. For example, when a player changes their name via the Player service, that change should ripple out to all interested parties so that interactions across devices can be updated with the new username.
We decided to try using Kafka, or, when we run in BlueMix, MessageHub (which has a Kafka interface, so essentially it&amp;#8217;s still Kafka to us).</description></item><item><title>Marco! Polo! (Service registration and discovery)</title><link>/chronicles/15-service-discovery.html</link><pubDate>Tue, 03 Mar 2020 17:29:15 -0500</pubDate><guid>/chronicles/15-service-discovery.html</guid><description>An item long on our wishlist was to stop having hardcoded endpoint urls (passed in as environment variables) for our services. We know it isn&amp;#8217;t nice, and it makes it harder to scale service instances without adding intermediate proxies all over the place.
We&amp;#8217;re using Amalgam8 (A8) for it&amp;#8217;s service discovery and service proxying abilities. Amalgam8 uses sidecars to manage service registration and discovery. Programmable load balancing and routing is provided via an in-container outbound proxy to other services.</description></item><item><title>Marco? Polo, Polo, Polo, Polo. (Caching).</title><link>/chronicles/16-caching.html</link><pubDate>Tue, 03 Mar 2020 17:29:15 -0500</pubDate><guid>/chronicles/16-caching.html</guid><description>Time passed&amp;#8230;&amp;#8203; the grue failed to eat us. So we decided to look into Caching.
Overview.There are places in the Game On code where we request data "service to service" and don&amp;#8217;t expect that data to change very often. Additionally, some of the places we request data from, have limits imposed (no more than x requests within y time period). To combat this, we added some of our own little Caches into the code to store the replies, so we didn&amp;#8217;t need to make too many repeat requests.</description></item><item><title>Making a Watson Alchemy Data News Room</title><link>/chronicles/17-watson-news.html</link><pubDate>Tue, 03 Mar 2020 17:29:15 -0500</pubDate><guid>/chronicles/17-watson-news.html</guid><description>This entry in the Chronicles will seem more like deleted scenes from a movie when compared to what has been chronicled so far.
The room I wanted to create was a Watson Alchemy Data API News Room. The details and the code can be found on github
Among other things, what slowed me down a tad bit was trying to look at why a simple string edit to initial values of the room description would cause the tests to fail.</description></item><item><title>Weather Room (Interacting with REST API service)</title><link>/chronicles/18-weather-room.html</link><pubDate>Tue, 03 Mar 2020 17:29:15 -0500</pubDate><guid>/chronicles/18-weather-room.html</guid><description>Game On! is desgined as a fun way to explore microservices, right? So our rooms are the microservice and we can write some puzzles, etc for them. But shouldn&amp;#8217;t room vistors be able to interact with other microservices? Maybe outside of Game On!?
I&amp;#8217;m re-learning some programming techniques after being on the operations side of things for quite a long time. So I was excited, challenged, and a little scared to start programming again.</description></item></channel></rss>